{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H24yDUiVP5-n"
      },
      "source": [
        "## Your Model üå±Gardenüå± Execution Environment\n",
        "\n",
        "Use this notebook to write a function that executes your model(s). Tag that function with the `@garden_pipeline` decorator.\n",
        "\n",
        "Garden will take this notebook and build a container with it. When Garden executes your `@garden_pipeline`, it will be like like you have just run all the cells of this notebook once. So you can install libraries with `!pip install` and your function can use those libraries. You can also define helper functions and constants to use in your `@garden_pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qww1_jOzP5S9"
      },
      "outputs": [],
      "source": [
        "# This cell imports everything you need from Garden, please DO NOT DELETE THIS CELL!\n",
        "from garden_ai.model_connectors import HFConnector\n",
        "from garden_ai import PipelineMetadata, garden_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0s7Bealdp8M"
      },
      "outputs": [],
      "source": [
        "# Import frameworks and packages as needed. \n",
        "# We've included 'sklearn' as the default ML framework.\n",
        "# Use '!pip install' or '%pip install' to install other libraries.\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import joblib # a Python library for running computationally intensive tasks in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aikfsCRdrEZ"
      },
      "source": [
        "### Model connectors\n",
        "\n",
        "Please make sure you have published your model on another service (e.g. Hugging Face) before using Garden! You will reference your model using our ***model connectors*** in this notebook. We have you host your model elsewhere to streamline your Garden experience; Garden is meant to put your work out there for the scientific community but we are not a model hosting service.\n",
        "\n",
        "***Model connectors*** let Garden import metadata about your model. They also have a `stage` method that you can use to download your model weights. \n",
        "<!-- Explain a bit more about the stage methid and why it's needed. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7em6SwMdvkt"
      },
      "outputs": [],
      "source": [
        "# Below is an example of how you reference a Hugging Face model. \n",
        "# It requires a Hugging Face repository ID, which is the name of your model repository following this format: (\"Owner/Model Name\").\n",
        "# You can copy this directly from your model repository page.\n",
        "\n",
        "my_hugging_face_repo = HFConnector(\"garden-ai/sample_sklearn_model\")\n",
        "\n",
        "# Feel free to reference as many models as you'd like in this cell.\n",
        "\n",
        "# TODO: Elaborate more on the optional parameters users can use for HFConnector: revision and local_dir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTziKOq7d1Qs"
      },
      "source": [
        "### Pipeline metadata\n",
        "\n",
        "Use the cell below to enter metadata for your pipeline function. Some information you can include are: function title, description, authors, and more.\n",
        "\n",
        "Why? --- This helps your model be more discoverable, contributes to open-science, and makes your work more replicable!\n",
        "\n",
        "Edit the PipelineMetadata object below to describe your pipeline function. Only one is needed for each pipeline function\n",
        "\n",
        "***PLEASE NOTE:*** The metadata you put in here is permanent and cannot be edited once the pipeline is published!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHtZD33NhCEF"
      },
      "outputs": [],
      "source": [
        "my_pipeline_meta = PipelineMetadata(\n",
        "    ######    REQUIRED    ######\n",
        "    title=\"My Inference Function\",\n",
        "    description=\"Write a longer description here so that people know what your pipeline function does.\",\n",
        "    authors=[\"you\", \"your collaborator\"],\n",
        "    tags=[\"materials science\", \"your actual field\"]\n",
        "\n",
        "    ######    OPTIONAL    ######\n",
        "    # TODO: Put in more entries\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnNDYs4PhKKO"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "Define any helper functions you need and use them in the function you want to let people run remotely (next cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhd9FNB9hN0a"
      },
      "outputs": [],
      "source": [
        "# The following function is an example of a helper function. Replace it with your own functions.\n",
        "def preprocess(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    input_df.fillna(0, inplace=True)\n",
        "    return input_df\n",
        "\n",
        "# TODO: Confirm if Models are automatically a step\n",
        "\n",
        "# You can mark any helper functions as a Garden Step. \n",
        "# If a function is marked as a Step, it will be shown on Garden's UI and represents code you want to highlight.\n",
        "# Steps are optional but are highly recommended to break down your code into more readable chunks.\n",
        "\n",
        "# To mark a function as a Garden Step, add the @garden_step() decorator above your function declaration like so:\n",
        "@garden_step(function_name=\"The name of the step function that will be used in Garden's UI\", description=\"An optional string describing the function.\")\n",
        "def example_step(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    input_df.fillna(0, inplace=True)\n",
        "    return input_df\n",
        "\n",
        "# What order are my steps in?\n",
        "# -> For now, there is no way to specify the order of your steps. \n",
        "# -> The order is always the order they are defined in your notebook, with the pipeline function always being the final step. \n",
        "# -> If there is more than one pipeline in a notebook, all the steps apply to both pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDPkWjAShSKr"
      },
      "source": [
        "### Write your pipeline function that will run remotely\n",
        "\n",
        "The `@garden_pipeline` decorator makes this function available to run in your garden when you publish the notebook.\n",
        "Make sure you keep the lines that are for downloading your model weights from hosting services and calling your model in this function.\n",
        "\n",
        "In the decorator be sure to include:\n",
        "- your pipeline metadata,\n",
        "- connectors for any models you're using,\n",
        "- the DOI of the garden you want this pipeline to be found in. (Check `garden-ai garden list` for the DOIs of your gardens.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ls-44Wehec9"
      },
      "outputs": [],
      "source": [
        "# Edit the function below as needed. This function is what we'll run when your pipeline is called, so pay extra attention to what your put in here!\n",
        "@garden_pipeline(metadata=my_pipeline_meta,  model_connectors=[my_hugging_face_repo], garden_doi=\"10.23677/my-garden-doi\")\n",
        "def run_my_model(input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    cleaned_df = preprocess(input_df) # Edit as needed with code and helper functions you declared in previous cells.\n",
        "    \n",
        "    ######   You may edit but not remove these lines! Garden needs this for your pipeline to run.   ######\n",
        "    download_path = my_hugging_face_repo.stage() # This downloads your model from HF and returns the path. Edit this for your model connector.\n",
        "    model = joblib.load(f\"{download_path}/model.pkl\") # This defines your model from the path we just got from the line above\n",
        "    return model.predict(cleaned_df) # Edit as needed for what your pipeline should run/output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK3PHq2fhgxp"
      },
      "source": [
        "### Test your pipeline function\n",
        "\n",
        "Finally, make sure your `@garden_pipeline` works!\n",
        "When Garden makes a container from your notebook, it runs all the cells in order and saves the notebook. Then users invoke your `@garden_pipeline` in the context of the notebook.\n",
        "\n",
        "If you can hit \"Kernel\" -> \"Restart and run all cells\" and your test below works, your `@garden_pipeline` will work in your garden!\n",
        "\n",
        "‚ö†Ô∏è Once you've verified your pipeline works, make sure to clear all output in this notebook, **delete** or **comment out** this testing section, and run everything again. This is to prevent your model weights from being included redundantly in this notebook. Your model weights will be downloaded via our model connectors when the pipeline is run remotely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn2aa8wnhu2u"
      },
      "outputs": [],
      "source": [
        "# Replace with input that is relevant for your garden_pipeline:\n",
        "example_input = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4],\n",
        "    'B': [None, 2, 3, 4],\n",
        "    'C': [1, 2, 3, 4]\n",
        "})\n",
        "\n",
        "# Run the @garden_pipeline function that you declared in the previous section:\n",
        "run_my_model(example_input)\n",
        "\n",
        "# TODO: What to do when the input is too complex? (e.g. images)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
